---
layout: post
title:  RL LLM
date:   2023-12-25 16:40:16
description: 
tags: thoughts
categories: thoughts
published: false
---


# Training Method of Large Language Models

## Pre-training and Supervised Instruction Tuning

In the pre-training of causal language models, we commonly use
cross-entropy loss to optimize the model, allowing it to accurately
predict the next word. In this case, let's say we have a sentence of
length $N$, where each word is represented as $x_i$, its true subsequent
word is $y_i$, and the model's predicted probability distribution of the
next word is $P(y_i | x_i)$. The cross-entropy loss can then be defined
as:

$$L = -\frac{1}{N}\sum_{i=1}^{N}\log P(y_i | x_i)$$

This loss function is the objective we strive to minimize during
training.

Then, in the supervised fine-tuning stage, the model can be thought of
as being retrained on a specific task, allowing it to better adapt to
specific question-answering tasks. This process can be seen as
optimizing a new loss function $L'$, similar to the original loss
function $L$, but it's calculated on the specific task dataset instead
of the original corpus.

$$L' = -\frac{1}{M}\sum_{j=1}^{M}\log P(y_j' | x_j')$$

Where $M$ is the size of the specific task dataset, and $x_j'$ and
$y_j'$ are the inputs and target outputs in the specific task dataset.

Overall, the goal is to find a set of model parameters $\theta$ that
perform well on as many tasks as possible, which can be achieved by
minimizing the following objective function:

$$\theta^* = \arg \min_\theta (\alpha L + (1 - \alpha)L')$$

Where $\alpha$ is a weight parameter used to balance the importance of
the original task and the specific task.

## Specific Well Defined Loss Function

Garg et al [@Garg2022] trained Transformer to solve a specific kind of
machine learning problems, and noticed that the behavior of the
experimental results of Transformer-based models are quite similar to
that of gradient descent. This phenomenon happens not only in linear
regression case, but also in LASSO regression, neural network
regression, and decision-tree based regression. However, their work only
investigated a specific regression solver, here we have to mention that
the loss function used there is MSE loss, which is well defined to be
the mean euclidean distance between two vectors.

It is obvious that through supervised instruction tuning, large language
model could help to solve all kinds of problems including mathematical
problems, statistics, formula derivation, and daily life problems, and
even reasoning. Generally speaking, we could define a generalized loss
function, which could represent the loss of all problems, not limited to
well defined problems above (like linear regression MSE loss).

$$\mathcal{L}_G (y, f(X, W))$$

## Super Loss Function: Introducing Reward Models

We name the proposed loss function $\mathcal{L}_G$ as \"super loss
function\". If we aim to create a \"super loss function\", it needs to
capture the correspondence between questions and answers and be able to
measure the correctness of the answers. This is a high-level goal
because it involves understanding and applying real-world knowledge.

Most loss functions we currently use, like cross-entropy loss, are
calculated at the word level and can only measure a surface-level
similarity between the model-generated answer and the real answer. To
create a loss function that can understand and evaluate the quality of
an answer, we might need to introduce some new techniques and concepts.

OpenAI [@Ouyang2022] proposed to use a reinforcement learning framework
to finetune their language model. In this case, we could define a reward
function $R(s, a)$, where $s$ represents the question and $a$ represents
the answer. This reward function can measure the quality of answer $a$
given question $s$. Then, our goal is to find a policy $\pi(a | s)$ that
can select the best answer $a$ given the question $s$.

This process can be represented by the following formula:

$$\pi^* = \arg \max_\pi E_{(s, a) \sim \pi}[R(s, a)]$$

Where $E$ represents expectation, and $\pi^*$ is the optimal policy we
want to find. Designing an appropriate reward function $R(s, a)$ is
challenging. OpenAI used human evaluators to provide feedback to better
constructed the \"super loss function\".

## Problem Solving Ability in Large Language Models

Let's consider a set of problems $P = {p_1, p_2, ..., p_N}$ and their
corresponding correct solutions $S = {s_1, s_2, ..., s_N}$.

For a given problem $p_i$ and a model's proposed solution $\hat{s_i}$,
the super loss function $\mathcal{L}_{G}$ measures the discrepancy
between the model's solution $\hat{s_i}$ and the correct solution $s_i$.
This is analogous to other loss functions, but it should capture the
quality of problem-solving more deeply.

The task of the language model in the problem-solving context is to find
a function $\mathcal{L}_{G}:=f(p; \theta)$ (parameterized by $\theta$)
that minimizes the expected super loss over all problems and their
solutions:

$$\theta^* = \arg \min_{\theta} \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{G} (p_i, s_i, f(p_i; \theta))$$

This equation suggests that our goal is to find the model parameters
theta that minimize the average super loss function. The model,
parameterized by these optimal parameters, should then be able to solve
problems effectively.

Indeed, the problem-solving capability of a language model can be seen
as related to finding an appropriate \"super loss function\". This is
because, when we train a language model, we are optimizing the model to
predict the next word as accurately as possible. This process can be
viewed as searching for an optimal solution in a large space of possible
outputs, which is the essence of an optimization problem.

More specifically, when we pose a question and expect the model to
provide an answer, the model needs to find the best one among all
possible answers. This \"best\" answer can be defined as the one that
minimizes our \"super loss function\". In this case, our \"super loss
function\" becomes a measure of the quality of an answer, and we can use
it to guide the model to give the best answer when answering questions.

On the other hand, problem-solving itself is an optimization problem.
When we try to solve a problem, we are typically looking for the best
solution, which requires us to find the optimal one among all possible
solutions. This process involves optimization: we need to find an
optimal solution that minimizes (or in some cases, maximizes) the
\"super loss function\".

# Agent Based Reinforcement Learning

The goal of learning how to achieve the maximum cumulative reward
through interactions with an environment. The main elements of
reinforcement learning include: agent, environment, action, state, and
reward.

**Agent** An agent is the decision-making entity. It can receive the
state of the environment, choose and execute actions according to some
policy, and then obtain rewards from the environment. The goal of the
agent is to maximize long-term rewards by learning the policy.

**Environment** The environment is the world or context in which the
agent is located. It provides a new state and reward according to the
action of the agent. The environment can be the actual physical world, a
computer-simulated environment, or even some abstract decision process.

**State** The state is a specific description of the environment,
containing all the information needed for the agent's decision-making.
In some cases, the environment may be fully observable, which means that
the current state contains all the historical information. However, in
partially observable environments, the current state may not provide all
the historical information.

**Action** Actions are operations that an agent can perform given a
state. The action set can be discrete or continuous. Executing an action
affects the environment, leading to changes in the state and possibly
generating rewards.

**Reward** Reward is feedback from the environment for the agent's
action, used to indicate the goodness of the action. Rewards can be
positive (indicating rewards) or negative (indicating punishments). The
goal of the agent is to find a policy that maximizes the cumulative
reward by learning.

**Reinforcement Learning Process and Goal** These concepts can be
expressed by the following formula. At each time step $t$, the agent
chooses an action $a_t$ according to the current state $s_t$ and its
policy $\pi$. After executing the action, the environment transitions to
a new state $s_{t+1}$ and provides a reward $r_t$.

In reinforcement learning, the agent's goal is to find a policy $\pi^*$
that maximizes the expected cumulative reward, which can be expressed
as:

$$\pi^* = \arg\max_\pi E\left[\sum_{t=0}^\infty \gamma^t r_t\right]$$

Here, $\gamma$ is the discount factor, used to balance immediate rewards
and future rewards in cumulative rewards. If $\gamma$ is close to 0, the
agent will focus more on immediate rewards; if $\gamma$ is close to 1,
the agent will focus more on future rewards.

# Auto-Reinforcement Learning in Large Language Models

## Language Models Are Agents

Language model trained using \"super loss function\", which was
discussed previously, could be viewed as an optimizer to solve a various
kinds of problems, due to extremely high generalizability, it could also
handle previously unseen tasks. Chain-of-Thought reasoning, introduced
by Wei et al [@wei2022chain], allows language models to do not only
output a direct answer but first give step-by-step analysis and then
give final answer. Inspired by Chain-of-Thought prompting and LangChain
[@chase2022], we designed a special prompt for language model to use
multiple steps of actions with parameter to finish a given task:

::: tcolorbox
You are now an assitant who are able to finish a task according to human
request.

You now have access to the following API functions:

**{actions_and_descriptions}**

Use the following format:

**Thought:** you should always think about what to do

**Action:** the action to take, should only be one of **{action_names}**

**Action Input:** the input to the action, please use JSON format, if no
input needed, use {}

**Observation:** the result of the action

\... (this Thought/Action/Action Input/Observation can repeat N times)

Here is an example of completion, you can refer but don't fully mimick
it:

**{example}**

Begin! Your TASK is: **{task}**

**{agent_playground}**
:::

Initially the language model will receive a grounded version of this
prompt template, with **{actions_and_descriptions}**,
**{action_names}**, **{example}**, and **{task}** filled. While the
**{agent_playground}** is left blank. The language model will output the
first set of response:

::: tcolorbox
**Thought:** I should first create a PowePoint slides.

**Action:** create_file

**Action Input:** {}

**Observation:**
:::

We deploy an externel program to extract the content behind **Action**
and **Action Input** respectively, and plug it into the function. After
the function is executed, an observation will be returned to the
language model itself, the observation is either a return value of
function or an exception description.

::: algorithm
::: algorithmic
Initialize large language model $f$ trained on super loss function
$\mathcal{L}_G$. Initialize context $C_{t=0} = [s_0]$, task, and
predefined action space $\{a_i\}$. Increment iteration count.
$(a_t, p_t) \leftarrow \pi(a_i, p_i | C_t)$ where $\pi$ is the policy
function. Execute action $a_t$ with parameters $p_t$ in the environment.
Observe $o_{t+1}$ from the environment. Update context:
$C_{t+1} = [s_{t+1}, o_1, o_2, ..., o_t, o_{t+1}]$. Preserve the final
context $C_{t=T}$ as a positive case $C_{+}$. Mark task as finished.
:::
:::

## Formulation of Auto-Reinforcement Learning

We formally define the concept of Auto-Reinforcement Learning here. Let
$f$ be a large language model trained on \"super loss function\"
$\mathcal{L}_G$ defined above. Define the context to be
$C_{t=T} = [s_T, o_1, o_2, ..., o_T]$, where $s_T$ is the state of the
environment at time $t=T$ and $o_t$ are observations. At each time step
$t=T$, the function $f$ accepts the context $C_t$, and choose a $a_t$
from a predefined action space ${a_i}$ and parameters for the this
action. The environment will produce an observation $o_t$ and add it to
the context.

Once the model finishes a given task, the context will be perserved to
$C_{+}$ as a positive case. The policy function $\pi(a_i|s_i)$ is not
explicitly updated so far in our case.

Let's denote $\pi(a_i, p_i | C_t)$ as the action with specified
parameters chosen by the function $\pi$ given the context $C_t$. The
process can be defined as:

1\. The model receives the current context
$C_t = [s_t, o_1, o_2, ..., o_t]$.

2\. The model chooses an action $a_t$ and parameters for this action
$p_t$, such that $(a_i, p_i) = \pi(a_i, p_i | C_t)$.

3\. The environment produces an observation $o_{t+1}$ in response to
action $a_t$ with parameters $p_t$.

4\. The context is updated:
$C_{t+1} = [s_{t+1}, o_1, o_2, ..., o_t, o_{t+1}]$.

The policy function isn't explicitly updated in this case, however the
model tried to maximize the value function $V = \mathcal{L}_G$.

When a given task is successfully completed by large language model, in
other words, the value function $V(C_t)$ is maximized. The final context
$C_{t=T}$ is preserved as a positive case $C_{+}$.

In our proposed approach, we employ a unique form of reinforcement
learning, termed as Auto-Reinforcement Learning, within the framework of
a Transformer model. Rather than explicitly learning a policy via reward
feedback, our model implicitly adjusts its behavior based on the
successful contexts from past actions.

In Transformer [@Vaswani2017] architecture, attention mechanisms play a
key role. The scaled dot-product attention mechanism used in these
models is expressed as:

$$\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

where $Q$ is the matrix of queries, $K$ is the matrix of keys, $V$ is
the matrix of values, $d_k$ is the dimension of the key vectors, and,
and the softmax function is applied row-wise.

The attention mechanism assigns different weights to different parts of
the input (context $C_t$) based on their relevance to the current task.
In the context of our Auto-Reinforcement Learning framework, changes to
the context---especially the addition of successful cases---can be seen
as implicit updates to the model's parameters.

To illustrate, when we add successful contexts (denoted as $C_{+}$) to
the current context, the queries, keys, and values are modified. These
are derived from the input tokens through separate learned linear
transformations, represented by $W_Q$, $W_K$, and $W_V$ respectively:

$$Q = C_{+}W_Q, \quad K = C_{+}W_K, \quad V = C_{+}W_V$$

This isn't a parameter update in the traditional sense: the actual
values of $W$ remain unchanged. Instead, we're changing the data that
$W$ is applied to, which can have a similar effect. This subtle form of
adaptation could have significant implications for the continued
training and performance of large language models. By using such
mechanism, we are implicitly updating the policy function $\pi(C_t)$ by
adding high-value examples to the context $C_{t=0}$ of new tasks. Each
time the model predicts the next token, the context will change the
semantic meaning of the problem, and the model minimizes the super loss
function $\mathcal{L}_G (C_0)$. The effect could be regarded as a update
of $\pi(a_i, p_i|C_t)$.

## Experiment

#### Experiment Setting

We want to verify the feasibility of Auto-Reinforcement Learning, so we
use 4 different settings: vanilla, with replay (add previous successful
contexts into the initial context of a new task), agent (use multiple
step action chain as described in previous theoretical formulation),
agent setting with replay. The experiment content is to test the
PowerPoint slides making ability of large language models.

#### Dataset

Owing to the absence of an existing dataset in this field, we resorted
to the use of a curated dataset. Initially, we prompted GPT-4 to
generate a list of 50 careers that necessitate proficiency in
slide-making. Subsequently, for each career seed, we tasked GPT-4 with
generating a comprehensive set of detailed instructions catering to the
slide-making requirements pertinent to each career. The dataset consists
of 100 specific slides making tasks.

#### Evaluation Criteria

The evaluation criteria is decomposed into two components:

**1. Non Exception Rate:** To make this criteria of fairness, we define
the success rate as follows: the user will always say one of 'It is you
who can decided.' or 'None', if the model succeed in generating at least
1 'pptx' file, the model succeeds. Otherwise, the model fails. This
handles the case when the models will query the user for some
clarification, which may lead to multiple chat steps.

**2. Relative Fulfillment Ranking:** To what extent that responses
reflect the given task descriptions. We sampled 10 tasks from 100-task
dataset, for each sampled task, we put generated slides under different
settings together, and manually give a ranking to each slides according
to the matching performance between the task description and the
generated slides. Finally, we average the relative rankings within a
specific setting. The best slides receive 1, slides with different
strength will receive the same rank, thus the worst rank is not always
8. For a specific task, if one setting failed to generate slides, we
will not consider when taking the mean. Finally we calculate the mean
rank of each setting. The lower the ranking, the better matching
performance is in the setting. By using relative ranking, we could make
sure the relative performance is persisted compared to assigning an
absolute score to each slides.

#### Results

| **Model**         | **Experiment Setting** | **Non Exception Rate** | **Relative Fulfillment Ranking** |
|-------------------|------------------------|------------------------|----------------------------------|
| text-davinci-003  | Vanilla                | 95%                    | 5.10                             |
| text-davinci-003  | + Agent                | 95%                    | 5.00                             |
| text-davinci-003  | + Agent + Replay       | 97%                    | 3.10                             |
| gpt-3.5-turbo     | Vanilla                | 57%                    | 2.28                             |
| gpt-3.5-turbo     | + Agent                | 94%                    | 3.11                             |
| gpt-3.5-turbo     | + Agent + Replay       | 86%                    | 1.50                             |


# Discussion

The results presented in Table 1 demonstrate that the proposed
Auto-Reinforcement Learning framework can improve the performance of
large language models in the task of generating PowerPoint slides.

The use of an 'Agent' setting, where a multiple-step action chain is
used, had a modest impact on the performance. This highlights the
importance of sequence-based decision-making in complex tasks such as
slide creation.

The results support our assertion that the Auto-Reinforcement Learning
approach can provide an effective strategy for guiding the behavior of
large language models. By implicitly learning from successful past
experiences, these models can make more contextually relevant decisions
and improve their performance over time.

#### Limitation

We have noticed that the large language model has seen slides making
tasks during pretraining, as it could directly generate slides making
scripts in 'Vanilla' setting, especially for 'text-davinci-003', the
non-exceptional rate reached 95%. This fact may introduce a weakness to
the experiment, as the model have knowledge about the task, so the
effectiveness of Auto-Renforcement Learning is not emphasized here. To
achive a more convincing results, we should consider using a completely
new task which the model hasn't seen before.


# Experiment Result Dataset

For the experiment result dataset (including 8\*100 successful contexts
and produced slides), see [Onedrive Share
Link](https://cuhko365-my.sharepoint.com/:u:/g/personal/119010355_link_cuhk_edu_cn/ESnAfQQxgfNEnrS0bdFTClsBJBCwCsULVZLtdQ5fxY46Og?e=nolfyz).
